# Vision Language Driving Perception ğŸš—ğŸ§ 

**Fine-tuning Vision-Language Models for Autonomous Driving Decision Planning**

---

## Overview

Vision Language Driving Perception is an open-source project focused on fine-tuning Vision-Language Models (VLMs) for decision planning in autonomous driving scenarios. By leveraging the expressive power of pre-trained VLMs, this project adapts them to downstream driving tasks such as behavior prediction, maneuver classification, and goal-directed planning.

This repository provides tools, datasets, and training pipelines to adapt InterVL2-1B for real-world autonomous driving decision modules.

---

## ğŸ”§ Features

- ğŸ§  **VLM Fine-Tuning Pipeline**: Modular pipeline to fine-tune VLM on driving-specific tasks.
- ğŸ“¦ **Dataset Integration**: Supports structured scene data (e.g., nuPlan, Waymo, or custom vectorized environments).
- ğŸ§¾ **Prompt Engineering for Driving Tasks**: Custom vision-language prompts for planning-relevant tasks.
- ğŸ§ª **Evaluation Tools**: Custom metrics for VLM output quality and scenario performance.

---

## ğŸ§© Use Cases

- Planning-aware scene understanding  
- Maneuver prediction with vision-language reasoning  
- Goal-directed trajectory selection  
- Safety-critical decision refinement using natural language context

---

## ğŸ“ Project Structure

```bash
Vision Language Driving Perception/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ setup.py
â”œâ”€â”€ command
â”‚   â”œâ”€â”€ InternVL2-1B.sh
â”‚   â””â”€â”€ eval.sh
â”œâ”€â”€ config
â”‚   â””â”€â”€ zero_stage1_config.json
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ sample.jsonl
â”‚   â””â”€â”€ vlm_samples
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ env_install.md
â”‚   â”œâ”€â”€ vlm_finetune.md
â”‚   â””â”€â”€ vlm_trt.md
â”œâ”€â”€ internvl
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conversation.py
â”‚   â”œâ”€â”€ dist_utils.py
â”‚   â”œâ”€â”€ model
â”‚   â”œâ”€â”€ patch
â”‚   â””â”€â”€ train
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ internvl_eval.py
â”‚   â”œâ”€â”€ pytorch_internvl_infer.py
â”‚   â””â”€â”€ trt_internvl_infer.py
â”œâ”€â”€ tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ arrow2jsonl.py
â”‚   â”œâ”€â”€ bart_score.py
â”‚   â”œâ”€â”€ convert_parquet.py
â”‚   â”œâ”€â”€ convert_to_int8.py
â”‚   â”œâ”€â”€ extract_mlp.py
â”‚   â”œâ”€â”€ extract_video_frames.py
â”‚   â”œâ”€â”€ extract_vit.py
â”‚   â”œâ”€â”€ json2jsonl.py
â”‚   â”œâ”€â”€ jsonl2jsonl.py
â”‚   â”œâ”€â”€ merge_lora.py
â”‚   â”œâ”€â”€ replace_llm.py
â”‚   â””â”€â”€ resize_pos_embed.py
â””â”€â”€ vlmtrt
    â”œâ”€â”€ build_vit_engine.py
    â”œâ”€â”€ conversation.py
    â””â”€â”€ convert_qwen2_ckpt.py
```

## ğŸš€ Get Started
### 1. Clone the repo
```bash
git clone https://github.com/thillai-c/Vision-Language-Driving-Perception.git
cd Vision-Language-Driving-Perception
```
### 2. Install dependencies
```bash
conda create -n vision-language-driving-perception python=3.10
conda activate vision-language-driving-perception
pip install -r requirements.txt
```
### 3. Prepare dataset
You can use scene data from nuPlan, Waymo, or a custom driving dataset. Please follow docs/data_prepare.md for instructions.

### 4. Run training
Please follow [docs/vlm_finetune.md](docs/vlm_finetune.md) to finetune vlms.
## ğŸ“Š Evaluation
Please follow [docs/vlm_finetune.md](docs/vlm_finetune.md) to evaluate your fine-tuned model on benchmark scenarios.

## ğŸ“¦ VLM Converter Module
The VLM Converter is a performance-boosting module designed to convert and quantize large Vision-Language Models (VLMs) using TensorRT-LLM, significantly improving inference speed while maintaining accuracy.

Please follow [docs/vlm_trt.md](docs/vlm_trt.md) to convert and quantize vlms.

## ğŸ§­ Acknowledgments
This project builds upon the work of:

[InterVL](https://github.com/OpenGVLab/InternVL)

[nuPlan](https://github.com/motional/nuplan-devkit)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)


[BLIP](https://github.com/salesforce/BLIP)

[LLaVA](https://github.com/haotian-liu/LLaVA)

And the broader open-source autonomous driving and VLM communities.